{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc842006",
   "metadata": {},
   "source": [
    "# Bullshit generator: a case study for n-gram language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85a4a4",
   "metadata": {},
   "source": [
    "## Finding n-grams!\n",
    "\n",
    "By now, you might have noticed that using one single word in the past to predict the next word feels wrong. This is because we choose words based on a long-term context - and using a single word is a large oversimplification on this.\n",
    "\n",
    "A possible solution is to change our original equation $ð‘ƒ(ð‘¤_ð‘›âˆ£ð‘¤_{ð‘›âˆ’1})$ to a less naive one in which the probability of a word is calculated based on $L$ previous words ($L$ stands for \"context length\"): $ð‘ƒ(ð‘¤_ð‘›âˆ£ð‘¤_{ð‘›âˆ’1}, w_{n-2}, \\cdots, w_{n-L})$ . For such, we will need to use n-grams.\n",
    "\n",
    "N-grams are simply sequences of N words that appear in the text. For example, in \"these are nice n-grams\", for n=2, we have the n-grams: \"these are\", \"are nice\", \"nice n-grams\". Note that now we can calculate $P(\\text{nice}|\\text{these are})$.\n",
    "\n",
    "We can get all n-grams and their continuations from a string using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7264ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is') -> my\n",
      "('is', 'my') -> cat\n",
      "('my', 'cat') -> this\n",
      "('cat', 'this') -> is\n",
      "('this', 'is') -> my\n",
      "('is', 'my') -> house\n",
      "('my', 'house') -> this\n",
      "('house', 'this') -> is\n",
      "('this', 'is') -> my\n",
      "('is', 'my') -> dog\n",
      "('my', 'dog') -> this\n",
      "('dog', 'this') -> is\n",
      "('this', 'is') -> my\n",
      "('is', 'my') -> computer\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def get_ngrams_and_continuations(input_str : str, L : int) -> tuple[list, list]:\n",
    "    list_of_words = re.findall(r'\\w+', input_str.lower())\n",
    "    ngrams = [tuple(list_of_words[i:i+L]) for i in range(len(list_of_words)-L)]\n",
    "    continuations = [list_of_words[i+L] for i in range(len(list_of_words)-L)]\n",
    "    return ngrams, continuations\n",
    "\n",
    "data = \"this is my cat. This is my house. This is my dog. This is my computer.\"\n",
    "ngrams, continuations = get_ngrams_and_continuations(data, 2)\n",
    "for i in range(len(ngrams)):\n",
    "    print(f\"{ngrams[i]} -> {continuations[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0593b4",
   "metadata": {},
   "source": [
    "## An N-Gram language model\n",
    "\n",
    "We can now proceed to estimate the probability of a continuation given an n-gram. For the example above, \"this is\" is definitely followed by \"my\". However, \"is my\" can be followed by \"cat\", \"house\", \"dog\", or \"computer\". Now, we can convert our n-grams and their continuations to a language model using probability counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e206ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is'): {'my': 1.0}\n",
      "('is', 'my'): {'cat': 0.25, 'house': 0.25, 'dog': 0.25, 'computer': 0.25}\n",
      "('my', 'cat'): {'this': 1.0}\n",
      "('cat', 'this'): {'is': 1.0}\n",
      "('my', 'house'): {'this': 1.0}\n",
      "('house', 'this'): {'is': 1.0}\n",
      "('my', 'dog'): {'this': 1.0}\n",
      "('dog', 'this'): {'is': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram_language_model(ngrams, continuations):\n",
    "    model = defaultdict(lambda: defaultdict(int))\n",
    "    for ngram, continuation in zip(ngrams, continuations):\n",
    "        model[ngram][continuation] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    for ngram, continuation_counts in model.items():\n",
    "        total_count = sum(continuation_counts.values())\n",
    "        for continuation in continuation_counts:\n",
    "            continuation_counts[continuation] /= total_count\n",
    "            \n",
    "    return model\n",
    "\n",
    "model = ngram_language_model(ngrams, continuations)\n",
    "for ngram, continuation_counts in model.items():\n",
    "    print(f\"{ngram}: {dict(continuation_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59c739",
   "metadata": {},
   "source": [
    "## Generating some bullshit\n",
    "\n",
    "Now, we can generate some bullshit by simply initializing our model with an N-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0229340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my cat this is my dog this is my house this is my house this is my house this is my dog this is my house this is my dog this is my dog this is my computer'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(41)  # For reproducibility\n",
    "\n",
    "initial_text = \"this is\"\n",
    "\n",
    "def generate_text(model, initial_text, n=2, length=10):\n",
    "    words = initial_text.split()\n",
    "    for _ in range(length):\n",
    "        ngram = tuple(words[-n:])\n",
    "        if ngram in model:\n",
    "            continuations = list(model[ngram].keys())\n",
    "            probabilities = list(model[ngram].values())\n",
    "            next_word = np.random.choice(continuations, p=probabilities)\n",
    "            words.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(words)\n",
    "\n",
    "generate_text(model, initial_text, n=2, length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21368d8d",
   "metadata": {},
   "source": [
    "## A fallback strategy\n",
    "\n",
    "Also, by now, you probably found out that larger n-grams become more and more uncommon. This is so true that finding two texts that contain n-grams with a context $L$ larger than around 10 can be used as basis to flag copy-paste plagiarism. Hence, with larger n-grams, we will probably fall into situations in which we don't have information on how to proceed.\n",
    "\n",
    "On the other hand, we might like larger context lengths because they can make our texts more cohesive.\n",
    "\n",
    "How to deal with that?\n",
    "\n",
    "One possibility is to have a weighting strategy in which the probabilities for models that consider different n-gram lengths are combined. However, the optimal combination could be hard to obtain.\n",
    "\n",
    "Another possibility is to use a fallback strategy: we try a model with context $L$. If it fails to find the n-gram, then we proceed to a model with context $L-1$, and so on.\n",
    "\n",
    "We could implement such a model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef5f6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my cat this is my house this is my dog this is my computer\n"
     ]
    }
   ],
   "source": [
    "models = {} # the key is the n-gram length L and the value is the model\n",
    "for L in range(1, 5):\n",
    "    ngrams, continuations = get_ngrams_and_continuations(data, L)\n",
    "    model = ngram_language_model(ngrams, continuations)\n",
    "    models[L] = model\n",
    "\n",
    "def generate_text_with_fallback(models, initial_text, max_length=40):\n",
    "    model_lengths = sorted(models.keys())[::-1]  # Start with the largest n-gram\n",
    "    words = initial_text.split()\n",
    "    for _ in range(max_length):\n",
    "        for L in model_lengths:\n",
    "            ngram = tuple(words[-L:])\n",
    "            if ngram in models[L]:\n",
    "                continuations = list(models[L][ngram].keys())\n",
    "                probabilities = list(models[L][ngram].values())\n",
    "                next_word = np.random.choice(continuations, p=probabilities)\n",
    "                words.append(next_word)\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(words)\n",
    "\n",
    "initial_text = \"this is\"\n",
    "np.random.seed(41)  # For reproducibility\n",
    "generated_text = generate_text_with_fallback(models, initial_text, max_length=40)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d424f57",
   "metadata": {},
   "source": [
    "## Generating some shakespeare!\n",
    "\n",
    "Well, now, let's get [Shakespeare's complete works](shakespeare.txt) and do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8062c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as file:\n",
    "    shakespeare_text = file.read()\n",
    "    \n",
    "models = {}\n",
    "for L in range(1, 7):\n",
    "    ngrams, continuations = get_ngrams_and_continuations(shakespeare_text, L)\n",
    "    model = ngram_language_model(ngrams, continuations)\n",
    "    models[L] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebc1d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe thyself than i will trust a sickly appetite that loathes even as it longs but sure my sister if i were ripe for your persuasion you have said enough to shake me from the arm of the all noble theseus\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(45)  # For reproducibility\n",
    "initial_text = \"I believe\"\n",
    "generated_text = generate_text_with_fallback(models, initial_text, max_length=40)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc3b8d",
   "metadata": {},
   "source": [
    "# Activities\n",
    "\n",
    "## Questions\n",
    "\n",
    "**Remembering (Recall facts and basic concepts)**\n",
    "\n",
    "1.  What is the main task being performed in this case study?\n",
    "2.  What machine learning algorithm is used for the classification task?\n",
    "3.  What technique is used to convert the texts into models?\n",
    "4.  What dataset is used for training and testing the model?\n",
    "5.  Do we need labels for this type of model?\n",
    "\n",
    "**Understanding (Explain ideas or concepts)**\n",
    "\n",
    "6.  Explain in your own words the core idea behind the N-Gram text representation.\n",
    "7.  What is the purpose of the fallback strategy in text generation?\n",
    "\n",
    "**Applying (Use information in new situations)**\n",
    "\n",
    "8. How would you modify the code for text generation to incorporate concepts like temperature, as we have seen previously?\n",
    "9. How can we use this same idea to generate movie reviews?\n",
    "10. How can we use this same idea to generate movie reviews that have positive sentiments?\n",
    "11. How can we use this same idea to generate movie reviews that have positive sentiments and mention the cinematography as positive?\n",
    "\n",
    "**Analyzing (Draw connections among ideas, compare/contrast, break down)**\n",
    "\n",
    "12. Analyze the outputs for shakespeare. Can you find the generated material within \"the complete works of Shakespeare\"?\n",
    "13. Is the model able to generate novel material, that is, phrases that have never been seen before?\n",
    "14. Can the model be considered \"creative\"?\n",
    "\n",
    "**Evaluating (Justify a stand or decision, critique)**\n",
    "\n",
    "15. Evaluate the author's statement: \"we have a reasonable reproduction of shakespeare\".\n",
    "16. Critique the interpretability of the model (predicting probability for single words). While insightful, what potential inaccuracies or simplifications does this method introduce compared to how words contribute within a text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f82a4",
   "metadata": {},
   "source": [
    "# Expected answers\n",
    "\n",
    "\n",
    "**Remembering (Recall facts and basic concepts)**\n",
    "\n",
    "1.  What is the main task being performed in this case study? Prediction of next word.\n",
    "2.  What machine learning algorithm is used for the classification task? N-Gram Language Models, or Order-N Markov Chains.\n",
    "3.  What technique is used to convert the texts into models? Simple counting and dividing.\n",
    "4.  What dataset is used for training and testing the model? The Complete Works of Shakespeare.\n",
    "5.  Do we need labels for this type of model? No.\n",
    "\n",
    "**Understanding (Explain ideas or concepts)**\n",
    "\n",
    "6.  Explain in your own words the core idea behind the N-Gram text representation. N-Grams are sequences of words that are considered a single \"token\". They can help modelling sequences of words.\n",
    "7.  What is the purpose of the fallback strategy in text generation? Larger N-Grams can become too rare. Hence, it can be necessary to resort to a lower-order model in some situations. The fallback strategy selects the higher-order model that can be used in each situation.\n",
    "\n",
    "**Applying (Use information in new situations)**\n",
    "\n",
    "8. How would you modify the code for text generation to incorporate concepts like temperature, as we have seen previously? Perhaps adding temperature before the choice. Another idea could be to assume all words are possible, with a minimum probability of $p$, so that we increase the chance of generating diverse outcomes.\n",
    "9. How can we use this same idea to generate movie reviews? Train the model in IMDB.\n",
    "10. How can we use this same idea to generate movie reviews that have positive sentiments? Train the model in the positive-label part of IMDB.\n",
    "11. How can we use this same idea to generate movie reviews that have positive sentiments and mention the cinematography as positive? Train the model in the items from IMDB that are positive and contain the word 'cinematography'.\n",
    "\n",
    "**Analyzing (Draw connections among ideas, compare/contrast, break down)**\n",
    "\n",
    "12. Analyze the outputs for shakespeare. Can you find the generated material within \"the complete works of Shakespeare\"? Some parts of the output are repeated, but larger chunks are more likely to be recombinations of smaller chunks from the original material, hence they cannot be found.\n",
    "13. Is the model able to generate novel material, that is, phrases that have never been seen before? More or less. It can create recombinations of known phrases and themes, but not entirely new themes and ideas.\n",
    "14. Can the model be considered \"creative\"? Not in a human sense. The generated novelties are simply the result of randomness. This is the same as flipping a coin many times - although that specific sequence of heads and tails could be new in the entire history of humanity, we would probably not argue that the coin was creative.\n",
    "\n",
    "**Evaluating (Justify a stand or decision, critique)**\n",
    "\n",
    "15. Critique the interpretability of the model (predicting probability for single words). While insightful, what potential inaccuracies or simplifications does this method introduce compared to how words contribute within a text? It is easy to interpret the model: in each step, we know what the model observes, and what are its possible outcomes. In fact, we could use debug messages to track all of this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
