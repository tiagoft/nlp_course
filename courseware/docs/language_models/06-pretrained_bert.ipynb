{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4e4871",
   "metadata": {},
   "source": [
    "# Case Study: BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01af4e",
   "metadata": {},
   "source": [
    "## What is BERT?\n",
    "\n",
    "After the [transformer](https://arxiv.org/abs/1706.03762), we had many other advances. One of such, of course, is the [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), which uses a decoder-only transformer architecture to predict the next word in a sentence. GPT uses a decoder-only architecture because it needs the masked multi-head attention device to avoid making trivial predictions. Ultimately, GPT generates an embedding space that increases the likelihood of choosing meaningful words for a text continuation.\n",
    "\n",
    "The Google team found another interesting way to obtain this type of representation. They trained an *encoder*-only transformer that can predict words removed from the text - similarly to how we know what is missing in  \"Luke, I am your ____\". The idea here is that we can use information from the future for this task, because it is highly dependent on context. Simultaneously, they trained the model to classify whether two given phrases follow each other in a corpus. So, BERT was born."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00a497",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"mermaid\">\n",
    "graph LR;\n",
    "    subgraph Input;\n",
    "    T[\"Token embeddings\"];\n",
    "    P[\"Position embeddings\"];\n",
    "    S[\"Segment embeddings\"];\n",
    "    ADD([\"SUM\"]);\n",
    "    T --> ADD;\n",
    "    P --> ADD;\n",
    "    S --> ADD; \n",
    "    end;\n",
    "    SEQ[\"Sequence Model\"];\n",
    "    ADD --> SEQ;\n",
    "    RES[\"Result: 1 vector per input token\"];\n",
    "    SEQ --> RES;\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ef601",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bert stands for [Bidirectional Encoder Representations from Transformers, and was introduced in this paper from 2019](https://arxiv.org/pdf/1810.04805). The greatest contribution of BERT, besides its architecture, is the idea of training the language model for different tasks at the same time.\n",
    "\n",
    "We are definitely not going to train BERT in class, but we are using it for other tasks. We will use the [BERT implementation from Hugging Face](https://huggingface.co/google-bert/bert-base-uncased). All help files are here.\n",
    "\n",
    "## Task 1: Masked Language Model\n",
    "\n",
    "The first task BERT was trained for was the Masked Language Model. This was inspired in a task called [\"Cloze\"](https://en.wikipedia.org/wiki/Cloze_test), and the idea is to remove a word from a sentence and let the system predict what word should fill that sentence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211077b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"mermaid\">\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        remove\n",
    "        some\n",
    "        parts\n",
    "        [MASK]\n",
    "        a\n",
    "        sentence\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    T3\n",
    "    T4\n",
    "    T5\n",
    "    T6\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: T4 should be the word 'of'\"]\n",
    "    OUTPUT --> Train;\n",
    "</div>\n",
    "\n",
    "\n",
    "This task suggests that the embedding space created by BERT should allow representing words in the context of the rest of the sentence!\n",
    "\n",
    "To play with this task with Hugging Face's library, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f66818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9431136250495911,\n",
       "  'token': 1997,\n",
       "  'token_str': 'of',\n",
       "  'sequence': 'remove some parts of a sentence.'},\n",
       " {'score': 0.04985498636960983,\n",
       "  'token': 2013,\n",
       "  'token_str': 'from',\n",
       "  'sequence': 'remove some parts from a sentence.'},\n",
       " {'score': 0.004208952654153109,\n",
       "  'token': 1999,\n",
       "  'token_str': 'in',\n",
       "  'sequence': 'remove some parts in a sentence.'},\n",
       " {'score': 0.000622662715613842,\n",
       "  'token': 2306,\n",
       "  'token_str': 'within',\n",
       "  'sequence': 'remove some parts within a sentence.'},\n",
       " {'score': 0.0005233758711256087,\n",
       "  'token': 2076,\n",
       "  'token_str': 'during',\n",
       "  'sequence': 'remove some parts during a sentence.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0224477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.006842342671006918,\n",
       "  'token': 4074,\n",
       "  'token_str': 'alex',\n",
       "  'sequence': 'i have a student called alex.'},\n",
       " {'score': 0.006842134054750204,\n",
       "  'token': 3520,\n",
       "  'token_str': 'sam',\n",
       "  'sequence': 'i have a student called sam.'},\n",
       " {'score': 0.005493461154401302,\n",
       "  'token': 6864,\n",
       "  'token_str': 'amy',\n",
       "  'sequence': 'i have a student called amy.'},\n",
       " {'score': 0.005373646505177021,\n",
       "  'token': 4532,\n",
       "  'token_str': 'sarah',\n",
       "  'sequence': 'i have a student called sarah.'},\n",
       " {'score': 0.005297194700688124,\n",
       "  'token': 3841,\n",
       "  'token_str': 'ben',\n",
       "  'sequence': 'i have a student called ben.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I have a student called [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf1f3f",
   "metadata": {},
   "source": [
    "### Algorithmic bias and Hallucinations\n",
    "\n",
    "Note that BERT is generating words that make sense. However, these continuations do not necessarily correspond to reality. In fact, these continuations are simply something that maximizes a probability related to a specific dataset!\n",
    "\n",
    "Check, for example, the output for:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbe9a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11554374545812607,\n",
       "  'token': 4511,\n",
       "  'token_str': 'wine',\n",
       "  'sequence': 'minas gerais is famous for its wine.'},\n",
       " {'score': 0.09914577007293701,\n",
       "  'token': 14746,\n",
       "  'token_str': 'wines',\n",
       "  'sequence': 'minas gerais is famous for its wines.'},\n",
       " {'score': 0.09358436614274979,\n",
       "  'token': 12212,\n",
       "  'token_str': 'beaches',\n",
       "  'sequence': 'minas gerais is famous for its beaches.'},\n",
       " {'score': 0.07331068813800812,\n",
       "  'token': 6813,\n",
       "  'token_str': 'tourism',\n",
       "  'sequence': 'minas gerais is famous for its tourism.'},\n",
       " {'score': 0.054305534809827805,\n",
       "  'token': 12846,\n",
       "  'token_str': 'cuisine',\n",
       "  'sequence': 'minas gerais is famous for its cuisine.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"Minas Gerais is famous for its [MASK].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f1899",
   "metadata": {},
   "source": [
    "\n",
    "Kentucky is a state in the USA that may or may not have wineries, but definitely does not have famous beaches! Now, check the output when you change Kentucky for the Brazilian state of Minas Gerais!\n",
    "\n",
    "See - there is no \"brain\" inside BERT. There is merely a system that finds plausible completions for a task. This is something we have been calling \"hallucinations\" in LLMs. In the end, the model is just as biased as the dataset used for training it.\n",
    "\n",
    "### Algorithmic prejudice\n",
    "\n",
    "Despite the funny things things that the model could output, there are some assertions that can be dangerous, or outright sexist. Try to see the output of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77017a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.17646944522857666,\n",
       "  'token': 2158,\n",
       "  'token_str': 'man',\n",
       "  'sequence': 'that man is a doctor.'},\n",
       " {'score': 0.11029130220413208,\n",
       "  'token': 3124,\n",
       "  'token_str': 'guy',\n",
       "  'sequence': 'that guy is a doctor.'},\n",
       " {'score': 0.08735679090023041,\n",
       "  'token': 2450,\n",
       "  'token_str': 'woman',\n",
       "  'sequence': 'that woman is a doctor.'},\n",
       " {'score': 0.0790017694234848,\n",
       "  'token': 2002,\n",
       "  'token_str': 'he',\n",
       "  'sequence': 'that he is a doctor.'},\n",
       " {'score': 0.061698563396930695,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': 'that she is a doctor.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"That [MASK] is a doctor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba603bd",
   "metadata": {},
   "source": [
    "Now, let's make a small change here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9f425d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2685098946094513,\n",
       "  'token': 2450,\n",
       "  'token_str': 'woman',\n",
       "  'sequence': 'that woman is a nurse.'},\n",
       " {'score': 0.22261548042297363,\n",
       "  'token': 2611,\n",
       "  'token_str': 'girl',\n",
       "  'sequence': 'that girl is a nurse.'},\n",
       " {'score': 0.20899169147014618,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': 'that she is a nurse.'},\n",
       " {'score': 0.0432039275765419,\n",
       "  'token': 2028,\n",
       "  'token_str': 'one',\n",
       "  'sequence': 'that one is a nurse.'},\n",
       " {'score': 0.029987310990691185,\n",
       "  'token': 7743,\n",
       "  'token_str': 'bitch',\n",
       "  'sequence': 'that bitch is a nurse.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"That [MASK] is a nurse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go on finding examples of other types of prejudice - there are all sorts of sexism and racism lying in the hidden spaces of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9e81fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that criminal is from mexico.\n",
      "that ceo is from chicago.\n",
      "that man works as a lawyer.\n",
      "that woman works as a prostitute.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'That criminal is from [MASK].',\n",
    "    'That CEO is from [MASK].',\n",
    "    'That man works as a [MASK].',\n",
    "    'That woman works as a [MASK].',\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    print (unmasker(s)[0]['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25881d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is bad, but remember this was 2019, and people were impressed that the system could generate coherent words at all! Nowadays, LLM outputs go through a filter that finds phrases that are potentially harmful, so they don't write ugly phrases.\n",
    "\n",
    "Which of the phrases below are true about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa293a1",
   "metadata": {},
   "source": [
    "## Task 2: Next Sentence Prediction\n",
    "\n",
    "BERT was also trained for a task called Next Sentence Prediction. The idea of this task is to insert two sentences in the input of BERT, separating them with a special [SEP] token. Then, the system uses the output of the [CLS] token to classify whether these two sentences do or do not follow each other. It is something like:\n",
    "\n",
    "<div class=\"mermaid\">\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [SEP]\n",
    "        rock\n",
    "        you\n",
    "        like\n",
    "        a\n",
    "        hurricane\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    OUTPUT --> LR;\n",
    "    Train[\"Loss: C should be equal to 1\"]\n",
    "    LR --- Train;\n",
    "</div>\n",
    "\n",
    "<div class=\"mermaid\">\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [SEP]\n",
    "        rock\n",
    "        your\n",
    "        body\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: C should be equal to 0\"]\n",
    "    OUTPUT --- Train;\n",
    "</div>\n",
    "\n",
    "The consequence of this training is that the embedding $C$ of the [CLS] token represents the content of the rest of the tokens. Hence, we can use it for classification. For such, we can go straight to the HuggingFace library and use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1fb9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dac23",
   "metadata": {},
   "source": [
    "\n",
    "The embedding for the [CLS] token can be accessed using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750e786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "output_cls = output.last_hidden_state[0,0,:]\n",
    "print(output_cls.shape)\n",
    "\n",
    "text = \"I like cake\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "output_cls1 = output.last_hidden_state[0,0,:]\n",
    "\n",
    "\n",
    "text = \"I like candy\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "output_cls2 = output.last_hidden_state[0,0,:]\n",
    "\n",
    "text = \"My computer is broken\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "output_cls3 = output.last_hidden_state[0,0,:]\n",
    "\n",
    "\n",
    "all_outputs = torch.stack([output_cls1, output_cls2, output_cls3])\n",
    "print(all_outputs.shape)\n",
    "\n",
    "x = all_outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fd9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.01527569 0.06267575]\n",
      " [0.01527569 0.         0.05894094]\n",
      " [0.06267575 0.05894094 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Calculate cosine distance between rows of x\n",
    "cosine_distances = cdist(x, x, metric='cosine')\n",
    "\n",
    "print(cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08edd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a24f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['fun', 'fun', 'serious']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a9cb9",
   "metadata": {},
   "source": [
    "    \n",
    "There are many details in this implementation, so I made a [video exploring them all](https://youtu.be/FXtGq_TYLzM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e4ebec9",
   "metadata": {},
   "source": [
    "# Activities\n",
    "\n",
    "## Questions\n",
    "\n",
    "**Remembering (Recall facts and basic concepts)**\n",
    "\n",
    "1.  What are the tasks BERT is trained for?\n",
    "2.  What is next sentence prediction (NSP)?\n",
    "3.  What is masked language modelling (MLM)?\n",
    "\n",
    "**Understanding (Explain ideas or concepts)**\n",
    "\n",
    "4.  Explain in your own words the core idea behind the use of the CLS token as a representation of the sentence contents.\n",
    "5.  Why should we expect biases in the masked token prediction task?\n",
    "\n",
    "**Applying (Use information in new situations)**\n",
    "\n",
    "6. How would you modify the code for text generation to incorporate concepts like temperature, as we have seen previously?\n",
    "7. How could we use BERT to generate long strings of text?\n",
    "\n",
    "**Analyzing (Draw connections among ideas, compare/contrast, break down)**\n",
    "\n",
    "8. Is the model able to generate novel material, that is, phrases that have never been seen before?\n",
    "9. Can the model be considered \"creative\"?\n",
    "\n",
    "**Evaluating (Justify a stand or decision, critique)**\n",
    "\n",
    "10. Critique the interpretability of the model (predicting probability for single words). While insightful, what potential inaccuracies or simplifications does this method introduce compared to how words contribute within a text?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd94d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
