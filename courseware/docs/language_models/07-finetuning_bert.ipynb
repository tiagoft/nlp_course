{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ddcf99",
   "metadata": {},
   "source": [
    "# Finetuning distilbert\n",
    "\n",
    "In this activity, we will finetune a pre-trained BERT model.\n",
    "\n",
    "The idea of fine tuning is that we will fit a classifier, as usual, but we will start with pre-trained weights.\n",
    "\n",
    "The process for fine-tuning is the same as \"fitting\" a model, except that we avoid the cold-start problem. In this activity, we will use DistilBert, which is a smaller version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e011fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3ea80",
   "metadata": {},
   "source": [
    "## Define a model\n",
    "\n",
    "First, we define a classifier using BERT. Note that the classifier uses the CLS token's output as its own input. If you don't remember what the CLS token is, go back to the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc64c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # Use the CLS token's output\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c55d2a",
   "metadata": {},
   "source": [
    "Now, we load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b784fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\").sample(50)\n",
    "# Sample size is here so that our demo is faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242693a",
   "metadata": {},
   "source": [
    "We will need a pre-trained tokenizer for distilbert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fe93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa5407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_df['label'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349f748",
   "metadata": {},
   "source": [
    "Now, note that we will need a `Dataset` and a `DataLoader`. The `Dataset` is a wrapper that allows sequentially retrieving items from our data. The `DataLoader` is another wrapper, which groups several items from a dataset into batches. Using them together allows us to avoid loading the whole dataset in our memory (which is useful when we have larger datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ca3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9629a",
   "metadata": {},
   "source": [
    "Now, note that we are going to be using an AdamW optimizer. AdamW builds on Adam by adding a regularization term, which prevents a phenomenon called \"catastrophic forgetting\". Catastrophic forgetting happens when the pre-trained model is fitted \"too much\" for fine tuning data, and the model starts losing its generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6913e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BertClassifier(bert_model=bert, num_classes=2, freeze_bert=False).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e4942",
   "metadata": {},
   "source": [
    "Training goes on as usual..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e315ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691f45c",
   "metadata": {},
   "source": [
    "... but we save the final model after fine-tuning (so that we can retrieve it later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8674ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(3):  # Train for some epochs...\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    all_losses.append(loss)\n",
    "\n",
    "torch.save(model.state_dict(), \"distilbert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e27ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(all_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.savefig(f\"distillbert_finetuning.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a9767",
   "metadata": {},
   "source": [
    "## Test the finetuned model\n",
    "\n",
    "Ok, now we can test the finetuned model in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278abb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"distilbert.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "test_df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/test-00000-of-00001.parquet\")\n",
    "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True, return_tensors='pt')\n",
    "test_labels = torch.tensor(test_df['label'].tolist())\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba95aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask, labels_batch = [x.to(device) for x in batch]\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_predictions.append(preds.cpu())\n",
    "\n",
    "all_predictions = torch.cat(all_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
