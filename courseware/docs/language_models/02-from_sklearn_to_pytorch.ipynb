{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Pytorch\n",
    "\n",
    "Allright, everyone. Let's go ahead and start doing things. We will start with a brief review, so that we are all on the same page.\n",
    "\n",
    "## Review: Logistic Regression with Scikit-Learn\n",
    "\n",
    "If you have reached this point, you probably know by heart how to define a Logistic Regression pipeline in Scikit Learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "classifier = LogisticRegression()\n",
    "pipeline = Pipeline([(\"vectorizer\", vectorizer),\n",
    "                        (\"classifier\", classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when we call `pipeline.fit()`, we first fit the vectorizer and then, with the results, we fit the classifier. So, these two methods for training our model are absolutely equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_the_pipeline(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def train_each_part_separately(vectorizer, classifier, X_train, y_train):\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "    return vectorizer, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we could go ahead and have our model trained, so that the next steps in this lesson are going to be a bit smoother. We could fit our model to classify some texts with our usual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/refs/heads/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Plot'], df['Genre'], test_size=0.2, random_state=42)\n",
    "vectorizer, classifier = train_each_part_separately(vectorizer, classifier, X_train, y_train)\n",
    "X_vect = vectorizer.transform(X_test)\n",
    "y_pred = classifier.predict(X_vect)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's recall how Logistic Regression works!\n",
    "\n",
    "The fitted predictor expects an input with $d$ features.\n",
    "\n",
    "We use the `vectorizer` to map each text in `X_test` to a vector with $d$ elements. Each of these vectors is a line $[x_1, x_2, x_3 \\cdots x_d]$ in `X_vect` (note that the index $d$ in $x_d$ is the same as the number of features expected by the fitted predictor.\n",
    "\n",
    "Then, Logistic Regression uses its fitter weights $\\beta$ to calculate a weighted sum of the elements in the input, that is:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\cdots + x_d \\beta_d.\n",
    "$$\n",
    "\n",
    "Note that if we have a matrix made of $N$ lines of features (each line corresponding to a dataset item!), then we can calculate the output for each line $i$ using:\n",
    "\n",
    "$$\n",
    "z_i = \\beta_0 + x_{i,1} \\beta_1 + x_{i,2} \\beta_2 + \\cdots + x_{i,d} \\beta_d.\n",
    "$$\n",
    "\n",
    "This can be translated into a matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "\\cdots \\\\\n",
    "z_N \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
    "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
    "                        \\end{bmatrix}\n",
    "                        \\begin{bmatrix}\n",
    "                        \\beta_1 \\\\\n",
    "                        \\beta_2 \\\\\n",
    "                        \\cdots \\\\\n",
    "                        \\beta_d\n",
    "                        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can simplify this by defining a weight matrix $w = [\\beta_1, \\beta_2, \\cdots, \\beta_d]$, and an input matrix $X$ containing all elements $x_{i,j}$ so that:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + X w^T\n",
    "$$\n",
    "\n",
    "Last, we apply the logistic function to each element of the vector $z$, and then we have a prediction.\n",
    "\n",
    "The elements of $z$ are called *logits*, $\\beta_0$ is called *bias* and the elements of $w$ are called *weights*.\n",
    "\n",
    "Good, but we didn't come this far to hear again about Logistic Regression. We now shall proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first model with Pytorch\n",
    "\n",
    "If you haven't done so, this is the moment to `pip install torch`. Pytorch is a framework that provides an API similar to Numpy with the addition of allowing operations in the GPU and easily using Autograd. Also, it contains many classes that are very useful for applications in Machine Learning. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "linear_layer = nn.Linear(in_features=3, out_features=1)\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + X w^T\n",
    "$$\n",
    "\n",
    "Sounds familiar? Let's see this operation working in practice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8340],\n",
      "        [-0.7707]], grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4499, -0.2000, -0.2288]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.1976], requires_grad=True)\n",
      "tensor([[-0.8340],\n",
      "        [-0.7707]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# We can call the linear_layer, and it will perform its operation:\n",
    "output = linear_layer(X)\n",
    "print(output)\n",
    "\n",
    "# We can also access the weights and biases of the linear layer:\n",
    "w = linear_layer.weight\n",
    "b = linear_layer.bias\n",
    "print(w, b)\n",
    "\n",
    "# We can use these weights and biases to perform the operation manually:\n",
    "z = X @ w.T + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_{1,1} & z_{1,2} & z_{1,3} \\\\\n",
    "z_{2,1} & z_{2,2} & z_{2,3} \\\\\n",
    "\\cdots \\\\\n",
    "z_{N, 1} & z_{N,2} & z_{N,3} \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
    "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
    "                        \\end{bmatrix}\n",
    "                        \\begin{bmatrix}\n",
    "                        \\beta_{1,1} & \\beta_{1,2} & \\beta{1,3}\\\\\n",
    "                        \\beta_{2,1} & \\beta_{2,2} & \\beta{2,3}\\\\\n",
    "                        \\cdots \\\\\n",
    "                        \\beta_{d,1} & \\beta_{d,2} & \\beta_{d,3}\n",
    "                        \\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + X w^T\n",
    "$$\n",
    "\n",
    "Sounds familiar? Let's see this operation working in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8340],\n",
      "        [-0.7707]], grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4499, -0.2000, -0.2288]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.1976], requires_grad=True)\n",
      "tensor([[-0.8340],\n",
      "        [-0.7707]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# We can call the linear_layer, and it will perform its operation:\n",
    "output = linear_layer(X)\n",
    "print(output)\n",
    "\n",
    "# We can also access the weights and biases of the linear layer:\n",
    "w = linear_layer.weight\n",
    "b = linear_layer.bias\n",
    "print(w, b)\n",
    "\n",
    "# We can use these weights and biases to perform the operation manually:\n",
    "z = X @ w.T + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_{1,1} & z_{1,2} & z_{1,3} \\\\\n",
    "z_{2,1} & z_{2,2} & z_{2,3} \\\\\n",
    "\\cdots \\\\\n",
    "z_{N, 1} & z_{N,2} & z_{N,3} \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
    "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
    "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
    "                        \\end{bmatrix}\n",
    "                        \\begin{bmatrix}\n",
    "                        \\beta_{1,1} & \\beta_{1,2} & \\beta{1,3}\\\\\n",
    "                        \\beta_{2,1} & \\beta_{2,2} & \\beta{2,3}\\\\\n",
    "                        \\cdots \\\\\n",
    "                        \\beta_{d,1} & \\beta_{d,2} & \\beta_{d,3}\n",
    "                        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "or, more compactly:\n",
    "\n",
    "$$\n",
    "z_{N \\times J} = x_{N \\times d} w^T_{d \\times j} + b_{1 \\times j}\n",
    "$$\n",
    "\n",
    "Note that $w \\in \\mathbb{R}^{j \\times d}$, hence $w^T \\in \\mathbb{R}^{d \\times j}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In the formulation for the linear layer above, $z$ corresponds to an important part of a logistic regression (remember: logistic regression has an input, a decision function, and then a final probability estimate $P(C | x)$). Which of these parts corresponds to the linear layer operation, and what is missing to make a full logistic regression using the linear layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear layer is a linear predictor?\n",
    "\n",
    "Allright, so the linear layer works essentially the same as the logistic regression. Let's show it!\n",
    "\n",
    "First, get back to the classifier we made in the beginning of this class. If you haven't fitted it yet, do it now.\n",
    "\n",
    "In the code below, we get the weights from the fitted logistic regression and substitute them in our linear layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_w = classifier.coef_\n",
    "logistic_b = classifier.intercept_\n",
    "\n",
    "w = torch.tensor(logistic_w, dtype=torch.float32)\n",
    "b = torch.tensor(logistic_b, dtype=torch.float32)\n",
    "\n",
    "linear_layer.weight.data = w\n",
    "linear_layer.bias.data = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the linear layer with `X_vect` and then apply a logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect_tensor = torch.tensor(X_vect.toarray(), dtype=torch.float32)\n",
    "output = linear_layer(X_vect_tensor)\n",
    "output_probs = torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our `output_probs` here is equal to the second column of the output of `classifier.predict_proba()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00112143 0.99887857]\n",
      " [0.52950462 0.47049538]\n",
      " [0.01416174 0.98583826]\n",
      " [0.07480901 0.92519099]\n",
      " [0.65259003 0.34740997]]\n",
      "tensor([[0.9989],\n",
      "        [0.4705],\n",
      "        [0.9858],\n",
      "        [0.9252],\n",
      "        [0.3474]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_probs = classifier.predict_proba(X_vect)\n",
    "print(y_probs[:5])\n",
    "print(output_probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we could simply decide our class by thresholding our output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n"
     ]
    }
   ],
   "source": [
    "binary_out = (output_probs > 0.5).numpy().astype(int)\n",
    "binary_y = (y_test==classifier.classes_[1]).astype(int)\n",
    "print(accuracy_score(binary_y, binary_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train a Logistic Regression\n",
    "\n",
    "Remember that our classifier outputs:\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\sigma(Xw^T + b),\n",
    "$$\n",
    "where $\\sigma()$ denotes the sigmoid (or logistic) function?\n",
    "\n",
    "The procedure to adjust the weights and biases in our linear layer is to use actual examples of outputs ($y$) and compare them to our estimate $\\hat{y}$. They are probably not going to be the same, so we can calculate how $y$ and $\\hat{y}$ are different using a loss function $L(y, \\hat{y})$. Then, we are going to calculate the derivative of $L$ with respect to all weights and biases, that is, we will have:\n",
    "\n",
    "$$\n",
    "g_i = \\frac{d L}{d p_i}\n",
    "$$\n",
    "\n",
    "for each parameter $p_i$ in the linear layer (either a weight or a bias). The parameters in the linear layer are the weights and biases. \n",
    "\n",
    "You might have noticed that $g$ is a vector of derivatives. Actually, you might even remember that it is called a *gradient* vector written as $\\nabla L$.\n",
    "\n",
    "Remember that the gradient points towards the direction in which $L$ grows the most if we change the parameters $p$? So, let's do the opposite! We want to decrease our loss, so we will change our parameters by *subtracting* a little bit of the gradient vector, that is, we iteractively update the parameters as:\n",
    "\n",
    "$$\n",
    "p_i \\leftarrow \\alpha \\frac{\\partial L}{\\partial p_i},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a small value of our choice. A lower $\\alpha$ means steps will be smaller, which can make it take longer to converge, whereas a higher value can lead to instabilities.\n",
    "\n",
    "We could be fancier and write this in vector form:\n",
    "\n",
    "$$\n",
    "p \\leftarrow \\alpha \\nabla_{\\mathbf{p}} L.\n",
    "$$\n",
    "\n",
    "What we usually do is to calculate the gradient $g$ for each item in the training data, and then sum (or average?) it before applying the step to the parameters. This process of going through the whole dataset is called *epoch*.\n",
    "\n",
    "Now, how do we implement this? Let's go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we prepare our data:\n",
    "X_vect_train = torch.tensor(vectorizer.transform(X_train).toarray(), dtype=torch.float32)\n",
    "y_train_ = (y_train == classifier.classes_[1]).astype(int).values\n",
    "y_train_vect = torch.tensor( y_train_, dtype=torch.float32).reshape( -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "Ok, now we are going to get into an optimization loop. When using Pytorch, we must first define an optimizer - we will use SGD, which is literally the gradient descent algorithm we have seen so far. Then, we will go into a training loop consisting of:\n",
    "\n",
    "1. Zeroing the gradient in the optimizer to reset its state,\n",
    "1. Calculate the output of the classifier\n",
    "1. Claculate the loss related to the output\n",
    "1. Calculate (going back into the layer!) the gradient of the loss using [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "1. Apply the gradient to our parameters\n",
    "\n",
    "It goes like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop\n",
      "201.01898193359375\n",
      "197.97332763671875\n",
      "195.9878387451172\n",
      "194.46324157714844\n",
      "193.14129638671875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's start with a new linear layer:\n",
    "clf = nn.Linear(in_features=len(vectorizer.vocabulary_), out_features=1)\n",
    "\n",
    "# We will also define an optimizer:\n",
    "optimizer = torch.optim.SGD(clf.parameters(), lr=1e-4) # lr is the learning rate - this is our alpha\n",
    "\n",
    "print(\"Entering loop\")\n",
    "# And now, this is the training loop:\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = clf(X_vect_train)\n",
    "    output_probs = torch.sigmoid(output)\n",
    "    loss = torch.sum( (output_probs-y_train_vect)**2 )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does pytorch know that gradients should be calculated? Well, they are calculated by default. That is why, in inference (or: in testing), we use `torch.no_grad()` and `model.eval()` - this saves memory, and saves computation time as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585\n"
     ]
    }
   ],
   "source": [
    "X_vect_test = torch.tensor(vectorizer.transform(X_test).toarray(), dtype=torch.float32)\n",
    "y_test_ = (y_test == classifier.classes_[1]).astype(int).values\n",
    "with torch.no_grad():\n",
    "    clf.eval()\n",
    "    y_pred = (torch.sigmoid(clf(X_vect_test)) > 0.5).numpy().astype(int)\n",
    "print(accuracy_score(y_test_, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching our training process\n",
    "\n",
    "If you simply increased the number of epochs to 1000 in our loop, you probably had an overflow of `print` statements with the current loss. We probably don't want to see that - rather, we want a figure!\n",
    "\n",
    "Up until a few years ago, we had to make this figure all by ourselves, with a procedure such as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 182.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # This will make us a progress bar\n",
    "\n",
    "clf = nn.Linear(in_features=len(vectorizer.vocabulary_), out_features=1)\n",
    "optimizer = torch.optim.SGD(clf.parameters(), lr=1e-4) # lr is the learning rate - this is our alpha\n",
    "\n",
    "# And now, a loop that is equal for everyone:\n",
    "losses = []\n",
    "for epoch in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    output = clf(X_vect_train)\n",
    "    output_probs = torch.sigmoid(output)\n",
    "    loss = torch.sum( (output_probs-y_train_vect)**2 )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nowadays, we have plenty of frameworks to store this same data: there is Weights and Biases, MLFlow, TensorBoard, and so on. Please, do feel free to use any of them. We will not adopt any of them for this course because we might get trapped within very specific details of them.\n",
    "\n",
    "\n",
    "**The main lessons here are:**\n",
    "\n",
    "* Save your data to a variable, and plot figures later (do NOT plot while doing training!)\n",
    "* Do not use the terminal to debug your loss\n",
    "* Use `tqdm` so we know your code is running.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Manipulate the code above to find the following:\n",
    "\n",
    "1. When we use `torch.sum( (output_probs-y_train_vect)**2 )` to calculate the loss, we are essentially saying that larger datasets have larger losses. Change this calculation so the loss is independent of the number of items in the dataset.\n",
    "1. You might want to increase the learning rate `lr` to make your training faster. What happens if you increase it too much? Can you guess why?\n",
    "1. Change your code to show that the `accuracy` (at least in the training set) tends to decrease together with the `loss`.\n",
    "1. How many epochs do you actually need in this training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
